{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf07e8d",
   "metadata": {},
   "source": [
    "# Commulyzer\n",
    "This notebook is used to train and export the toxicity classifiers on the cleaned Reddit comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4951a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keils\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cleaned dataset: data\\processed\\merged\\merged_comments_labeled_cleaned.csv\n",
      "Using labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'racism']\n"
     ]
    }
   ],
   "source": [
    "import json, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "except Exception as e:\n",
    "    print(\"iterative-stratification not found. Install with: pip install iterative-stratification\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                              Trainer, TrainingArguments, set_seed)\n",
    "    HF_AVAILABLE = True\n",
    "except Exception:\n",
    "    HF_AVAILABLE = False\n",
    "\n",
    "DATA_PROCESSED = Path(\"data/processed\")\n",
    "MERGED_DIR = DATA_PROCESSED / \"merged\"\n",
    "LABELED_DATA_PATH = MERGED_DIR / \"merged_comments_labeled_cleaned.csv\"\n",
    "OUTPUTS = Path(\"outputs\"); MODELS = OUTPUTS / \"models\"; REPORTS = OUTPUTS / \"reports\"\n",
    "for p in [DATA_PROCESSED, MERGED_DIR, OUTPUTS, MODELS, REPORTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not LABELED_DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cleaned dataset not found: {LABELED_DATA_PATH}. Run merge/label/clean scripts first.\"\n",
    "    )\n",
    "\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\",\"racism\"]\n",
    "print(\"Using cleaned dataset:\", LABELED_DATA_PATH)\n",
    "print(\"Using labels:\", LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c16b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def safe_read_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, dtype=str, keep_default_na=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: str):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"Saved: {path} ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a06290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset from data\\processed\\merged\\merged_comments_labeled_cleaned.csv ...\n",
      "Prepared rows: 893525\n",
      "Merged shape: (893525, 38)\n",
      "Prepared rows: 893525\n",
      "Merged shape: (893525, 38)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_rank</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_iso</th>\n",
       "      <th>...</th>\n",
       "      <th>racism_bin</th>\n",
       "      <th>labels</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>racism</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StellaSora</td>\n",
       "      <td>1olnny0</td>\n",
       "      <td>187</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>StellaSora</td>\n",
       "      <td>nmobnux</td>\n",
       "      <td>t1_nmjancc</td>\n",
       "      <td>98NINJA98</td>\n",
       "      <td>1762073636.0</td>\n",
       "      <td>2025-11-02T08:53:56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxic|obscene</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lmao 99% of the population don't play stella s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StellaSora</td>\n",
       "      <td>1olnny0</td>\n",
       "      <td>187</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>StellaSora</td>\n",
       "      <td>nml3c3l</td>\n",
       "      <td>t1_nmj7b1l</td>\n",
       "      <td>avelineaurora</td>\n",
       "      <td>1762026690.0</td>\n",
       "      <td>2025-11-01T19:51:30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxic|obscene</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They haven't even done any fixes yet... It's l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StellaSora</td>\n",
       "      <td>1olnny0</td>\n",
       "      <td>187</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>StellaSora</td>\n",
       "      <td>nmk65tt</td>\n",
       "      <td>t1_nmjgnvk</td>\n",
       "      <td>TankedCat</td>\n",
       "      <td>1762016350.0</td>\n",
       "      <td>2025-11-01T16:59:10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>toxic|obscene</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>honestly the ‘war on yuri’ is rooted in the pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_subreddit  post_id post_rank post_title   subreddit comment_id  \\\n",
       "0       StellaSora  1olnny0       187    Revenue  StellaSora    nmobnux   \n",
       "1       StellaSora  1olnny0       187    Revenue  StellaSora    nml3c3l   \n",
       "2       StellaSora  1olnny0       187    Revenue  StellaSora    nmk65tt   \n",
       "\n",
       "    parent_id         author   created_utc          created_iso  ...  \\\n",
       "0  t1_nmjancc      98NINJA98  1762073636.0  2025-11-02T08:53:56  ...   \n",
       "1  t1_nmj7b1l  avelineaurora  1762026690.0  2025-11-01T19:51:30  ...   \n",
       "2  t1_nmjgnvk      TankedCat  1762016350.0  2025-11-01T16:59:10  ...   \n",
       "\n",
       "  racism_bin         labels toxic severe_toxic obscene threat insult  \\\n",
       "0          0  toxic|obscene     1            0       1      0      0   \n",
       "1          0  toxic|obscene     1            0       1      0      0   \n",
       "2          0  toxic|obscene     1            0       1      0      0   \n",
       "\n",
       "  identity_hate racism                                         body_clean  \n",
       "0             0      0  Lmao 99% of the population don't play stella s...  \n",
       "1             0      0  They haven't even done any fixes yet... It's l...  \n",
       "2             0      0  honestly the ‘war on yuri’ is rooted in the pl...  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    need_cols = [\"source_subreddit\",\"post_id\",\"comment_id\",\"created_utc\",\"score\",\"body\",\"permalink\",\"post_rank\",\"post_title\",\"subreddit\"]\n",
    "    for c in need_cols:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "    for lab in LABELS:\n",
    "        if lab not in df.columns and f\"{lab}_bin\" in df.columns: df[lab] = df[f\"{lab}_bin\"]\n",
    "        if lab not in df.columns: df[lab] = \"0\"\n",
    "        df[lab] = df[lab].astype(str).str.extract(r\"(\\d)\").fillna(\"0\").astype(int).clip(0,1)\n",
    "\n",
    "    df[\"body\"] = df[\"body\"].astype(str)\n",
    "    df = df[~df[\"body\"].isin([\"\", \"[deleted]\", \"[removed]\"])].copy()\n",
    "    df[\"body_clean\"] = df[\"body\"].map(normalize_text)\n",
    "    df = df[df[\"body_clean\"] != \"\"]\n",
    "    df = df[~df[\"body_clean\"].str.startswith(\"http\")].copy()\n",
    "\n",
    "    if \"comment_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"comment_id\"])\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"body_clean\"])\n",
    "    return df\n",
    "\n",
    "def load_clean_dataset(path: Path) -> pd.DataFrame:\n",
    "    print(f\"Loading cleaned dataset from {path} ...\")\n",
    "    df = safe_read_csv(str(path))\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No rows found in {path}. Ensure the file contains labeled comments.\")\n",
    "    prepared = prepare_frame(df)\n",
    "    print(f\"Prepared rows: {len(prepared)}\")\n",
    "    return prepared\n",
    "\n",
    "merged = load_clean_dataset(LABELED_DATA_PATH)\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5904ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=599955, val=144616, test=148954\n",
      "Saved: data\\processed\\train.csv (599955 rows)\n",
      "Saved: data\\processed\\train.csv (599955 rows)\n",
      "Saved: data\\processed\\val.csv (144616 rows)\n",
      "Saved: data\\processed\\val.csv (144616 rows)\n",
      "Saved: data\\processed\\test.csv (148954 rows)\n",
      "Saved: data\\processed\\test.csv (148954 rows)\n"
     ]
    }
   ],
   "source": [
    "def thread_stratified_split(df: pd.DataFrame, label_cols: List[str], group_col: str=\"post_id\",\n",
    "                            val_size: float=0.15, test_size: float=0.15, seed: int=42):\n",
    "    agg = df.groupby(group_col)[label_cols].max().reset_index()\n",
    "    X = np.arange(len(agg)).reshape(-1,1)\n",
    "    Y = agg[label_cols].values\n",
    "\n",
    "    # Train vs temp\n",
    "    n_splits = max(3, int(1/(test_size+val_size)))\n",
    "    mskf1 = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    train_idx, temp_idx = next(mskf1.split(X, Y))\n",
    "\n",
    "    temp_X, temp_Y = X[temp_idx], Y[temp_idx]\n",
    "    mskf2 = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=seed+1)\n",
    "    val_rel, test_rel = next(mskf2.split(temp_X, temp_Y))\n",
    "    val_idx, test_idx = temp_idx[val_rel], temp_idx[test_rel]\n",
    "\n",
    "    threads = agg[group_col].values\n",
    "    train_threads, val_threads, test_threads = set(threads[train_idx]), set(threads[val_idx]), set(threads[test_idx])\n",
    "\n",
    "    pick = lambda s: df[df[group_col].isin(s)].copy()\n",
    "    train_df, val_df, test_df = pick(train_threads), pick(val_threads), pick(test_threads)\n",
    "    print(f\"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "df = merged.copy()\n",
    "for lab in LABELS: df[lab] = df[lab].astype(int)\n",
    "\n",
    "train_df, val_df, test_df = thread_stratified_split(df, LABELS, group_col=\"post_id\")\n",
    "save_df(train_df, DATA_PROCESSED / \"train.csv\")\n",
    "save_df(val_df,   DATA_PROCESSED / \"val.csv\")\n",
    "save_df(test_df,  DATA_PROCESSED / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7ded4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calibrate_thresholds(y_true: np.ndarray, y_prob: np.ndarray, label_names: List[str]) -> Dict[str, float]:\n",
    "    thresholds = {}\n",
    "    for i, lab in enumerate(label_names):\n",
    "        best_t, best_f1 = 0.5, -1.0\n",
    "        for t in np.linspace(0.05, 0.95, 19):\n",
    "            preds = (y_prob[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], preds, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        thresholds[lab] = float(best_t)\n",
    "    return thresholds\n",
    "\n",
    "def apply_thresholds(y_prob: np.ndarray, thresholds: Dict[str, float], label_names: List[str]) -> np.ndarray:\n",
    "    out = np.zeros_like(y_prob, dtype=int)\n",
    "    for i, lab in enumerate(label_names):\n",
    "        out[:, i] = (y_prob[:, i] >= thresholds.get(lab, 0.5)).astype(int)\n",
    "    return out\n",
    "\n",
    "def evaluate_all(y_true, y_prob, label_names, thresholds=None, title=\"\"):\n",
    "    if thresholds is None:\n",
    "        thresholds = {lab:0.5 for lab in label_names}\n",
    "    y_pred = apply_thresholds(y_prob, thresholds, label_names)\n",
    "\n",
    "    lines, ap_per, roc_per = [], [], []\n",
    "    for i, lab in enumerate(label_names):\n",
    "        ap  = average_precision_score(y_true[:, i], y_prob[:, i]) if y_true[:, i].sum() > 0 else float(\"nan\")\n",
    "        roc = roc_auc_score(y_true[:, i], y_prob[:, i]) if len(np.unique(y_true[:, i]))>1 else float(\"nan\")\n",
    "        p = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        r = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        f = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        ap_per.append(ap); roc_per.append(roc)\n",
    "        lines.append(f\"{lab:15s}  AP={ap:.3f}  ROC-AUC={roc:.3f}  P={p:.3f} R={r:.3f} F1={f:.3f} thr={thresholds[lab]:.2f}\")\n",
    "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    summary = f\"\\n{title}\\nMicro-F1={micro_f1:.3f}  Macro-F1={macro_f1:.3f}\\n\" + \"\\n\".join(lines)\n",
    "    print(summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4455572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Baseline] Test metrics (calibrated)\n",
      "Micro-F1=0.957  Macro-F1=0.858\n",
      "toxic            AP=0.976  ROC-AUC=0.997  P=0.956 R=0.959 F1=0.957 thr=0.65\n",
      "severe_toxic     AP=0.852  ROC-AUC=1.000  P=0.786 R=0.805 F1=0.795 thr=0.50\n",
      "obscene          AP=0.980  ROC-AUC=0.998  P=0.963 R=0.981 F1=0.972 thr=0.65\n",
      "threat           AP=0.762  ROC-AUC=0.999  P=0.799 R=0.633 F1=0.706 thr=0.95\n",
      "insult           AP=0.968  ROC-AUC=0.998  P=0.931 R=0.960 F1=0.945 thr=0.75\n",
      "identity_hate    AP=0.814  ROC-AUC=0.980  P=0.869 R=0.629 F1=0.730 thr=0.65\n",
      "racism           AP=0.906  ROC-AUC=0.988  P=0.905 R=0.901 F1=0.903 thr=0.45\n",
      "Baseline artifacts saved to outputs\\models\\baseline\n",
      "Baseline artifacts saved to outputs\\models\\baseline\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all(y_true, y_prob, label_names, thresholds=None, title=\"\"):\n",
    "\n",
    "    if thresholds is None:\n",
    "\n",
    "        thresholds = {lab: 0.5 for lab in label_names}\n",
    "\n",
    "    y_pred = apply_thresholds(y_prob, thresholds, label_names)\n",
    "\n",
    "\n",
    "\n",
    "    lines, ap_per, roc_per = [], [], []\n",
    "\n",
    "    for i, lab in enumerate(label_names):\n",
    "\n",
    "        y_true_col = y_true[:, i]\n",
    "\n",
    "        y_prob_col = y_prob[:, i]\n",
    "\n",
    "        ap = average_precision_score(y_true_col, y_prob_col) if y_true_col.sum() > 0 else float(\"nan\")\n",
    "\n",
    "        roc = roc_auc_score(y_true_col, y_prob_col) if len(np.unique(y_true_col)) > 1 else float(\"nan\")\n",
    "\n",
    "        p = precision_score(y_true_col, y_pred[:, i], zero_division=0)\n",
    "\n",
    "        r = recall_score(y_true_col, y_pred[:, i], zero_division=0)\n",
    "\n",
    "        f = f1_score(y_true_col, y_pred[:, i], zero_division=0)\n",
    "\n",
    "        ap_per.append(ap)\n",
    "\n",
    "        roc_per.append(roc)\n",
    "\n",
    "        lines.append(f\"{lab:15s}  AP={ap:.3f}  ROC-AUC={roc:.3f}  P={p:.3f} R={r:.3f} F1={f:.3f} thr={thresholds[lab]:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    subset_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    subset_err = 1.0 - subset_acc\n",
    "\n",
    "    summary = (\n",
    "\n",
    "        f\"\\n{title}\\n\"\n",
    "\n",
    "        f\"Micro-F1={micro_f1:.3f}  Macro-F1={macro_f1:.3f}  \"\n",
    "\n",
    "        f\"Subset-Acc={subset_acc:.3f}  Subset-Err={subset_err:.3f}\\n\"\n",
    "\n",
    "        + \"\\n\".join(lines)\n",
    "\n",
    "    )\n",
    "\n",
    "    print(summary)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "read_kwargs = dict(dtype=str, keep_default_na=False)\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\", **read_kwargs)\n",
    "\n",
    "val_df = pd.read_csv(DATA_PROCESSED / \"val.csv\", **read_kwargs)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\", **read_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "for split_name, frame in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "\n",
    "    missing = frame[\"body_clean\"].eq(\"\").sum() + frame[\"body_clean\"].isna().sum()\n",
    "\n",
    "    if missing:\n",
    "\n",
    "        print(f\"[{split_name}] dropping {missing} rows with empty body_clean\")\n",
    "\n",
    "        frame.drop(frame[frame[\"body_clean\"].isna() | frame[\"body_clean\"].eq(\"\")].index, inplace=True)\n",
    "\n",
    "    frame[\"body_clean\"] = frame[\"body_clean\"].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "Y_train = train_df[LABELS].values.astype(int)\n",
    "\n",
    "Y_val = val_df[LABELS].values.astype(int)\n",
    "\n",
    "Y_test = test_df[LABELS].values.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9, sublinear_tf=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df[\"body_clean\"])\n",
    "\n",
    "X_val = vectorizer.transform(val_df[\"body_clean\"])\n",
    "\n",
    "X_test = vectorizer.transform(test_df[\"body_clean\"])\n",
    "\n",
    "\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=4000, class_weight=\"balanced\"))\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "val_prob = clf.predict_proba(X_val)\n",
    "\n",
    "thresholds = calibrate_thresholds(Y_val, val_prob, LABELS)\n",
    "\n",
    "test_prob = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "report = evaluate_all(Y_test, test_prob, LABELS, thresholds, title=\"[Baseline] Test metrics (calibrated)\")\n",
    "\n",
    "Path(\"outputs/reports\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"outputs/reports/baseline_report.txt\").write_text(report, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "# Save artifacts\n",
    "\n",
    "model_dir = Path(\"outputs/models/baseline\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, model_dir / \"vectorizer.joblib\")\n",
    "\n",
    "joblib.dump(clf, model_dir / \"ovr_lr.joblib\")\n",
    "\n",
    "(model_dir / \"labels.txt\").write_text(\"\\n\".join(LABELS), encoding=\"utf-8\")\n",
    "\n",
    "(model_dir / \"thresholds.json\").write_text(json.dumps(thresholds, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Baseline artifacts saved to\", model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keils\\AppData\\Local\\Temp\\ipykernel_15548\\1639669508.py:22: RuntimeWarning: invalid value encountered in divide\n",
      "  pos_weight = (total - pos) / np.clip(pos, 1.0, None)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     metrics[\u001b[33m\"\u001b[39m\u001b[33mmacro_f1@0.5\u001b[39m\u001b[33m\"\u001b[39m] = f1_score(labels, preds, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m, zero_division=\u001b[32m0\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/models/transformer/checkpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicro_f1@0.5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBCELossTrainer\u001b[39;00m(Trainer):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "print(\"HF available:\", HF_AVAILABLE)\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"; max_len = 256; batch_size = 16; epochs = 3; lr = 2e-5\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "    class ToxDataset(Dataset):\n",
    "\n",
    "        def __init__(self, df):\n",
    "\n",
    "            self.texts = df[\"body_clean\"].tolist()\n",
    "\n",
    "            self.labels = df[LABELS].values.astype(float)\n",
    "\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "\n",
    "            return len(self.texts)\n",
    "\n",
    "\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            enc = tokenizer(\n",
    "\n",
    "                self.texts[idx],\n",
    "\n",
    "                truncation=True,\n",
    "\n",
    "                padding=\"max_length\",\n",
    "\n",
    "                max_length=max_len,\n",
    "\n",
    "                return_tensors=\"pt\",\n",
    "\n",
    "            )\n",
    "\n",
    "            item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "            return item\n",
    "\n",
    "\n",
    "\n",
    "    train_ds, val_ds, test_ds = ToxDataset(train_df), ToxDataset(val_df), ToxDataset(test_df)\n",
    "\n",
    "\n",
    "\n",
    "    pos = train_df[LABELS].sum(axis=0).values.astype(float)\n",
    "\n",
    "    total = len(train_df)\n",
    "\n",
    "    pos_weight = (total - pos) / np.clip(pos, 1.0, None)\n",
    "\n",
    "    pos_weight = np.clip(pos_weight, 1.0, 20.0)\n",
    "\n",
    "    pos_weight_t = torch.tensor(pos_weight, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "        model_name, num_labels=len(LABELS), problem_type=\"multi_label_classification\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    def custom_loss(outputs, labels):\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "\n",
    "            logits, labels, pos_weight=pos_weight_t.to(logits.device)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "\n",
    "        logits, labels = eval_pred\n",
    "\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "        label_int = labels.astype(int)\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for i, lab in enumerate(LABELS):\n",
    "\n",
    "            label_col = label_int[:, i]\n",
    "\n",
    "            if label_col.sum() > 0:\n",
    "\n",
    "                ap = average_precision_score(label_col, probs[:, i])\n",
    "\n",
    "                metrics[f\"ap_{lab}\"] = ap\n",
    "\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "        metrics[\"micro_f1@0.5\"] = f1_score(label_int, preds, average=\"micro\", zero_division=0)\n",
    "\n",
    "        metrics[\"macro_f1@0.5\"] = f1_score(label_int, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "        subset_accuracy = np.mean(np.all(preds == label_int, axis=1))\n",
    "\n",
    "        metrics[\"subset_accuracy@0.5\"] = subset_accuracy\n",
    "\n",
    "        metrics[\"subset_error@0.5\"] = 1.0 - subset_accuracy\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    base_training_kwargs = dict(\n",
    "\n",
    "        output_dir=\"outputs/models/transformer/checkpoints\",\n",
    "\n",
    "        learning_rate=lr,\n",
    "\n",
    "        per_device_train_batch_size=batch_size,\n",
    "\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "\n",
    "        weight_decay=0.01,\n",
    "\n",
    "        logging_steps=50,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "\n",
    "            **base_training_kwargs,\n",
    "\n",
    "            evaluation_strategy=\"epoch\",\n",
    "\n",
    "            save_strategy=\"epoch\",\n",
    "\n",
    "            load_best_model_at_end=True,\n",
    "\n",
    "            metric_for_best_model=\"micro_f1@0.5\",\n",
    "\n",
    "            report_to=\"none\",\n",
    "\n",
    "        )\n",
    "\n",
    "    except TypeError as exc:\n",
    "\n",
    "        print(\n",
    "\n",
    "            \"TrainingArguments fallback: transformers version lacks evaluation/save strategy kwargs; \"\n",
    "\n",
    "            \"continuing with minimal configuration.\"\n",
    "\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(**base_training_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    class BCELossTrainer(Trainer):\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "            labels = inputs.pop(\"labels\")\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = custom_loss(outputs, labels)\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "    trainer = BCELossTrainer(\n",
    "\n",
    "        model=model,\n",
    "\n",
    "        args=training_args,\n",
    "\n",
    "        train_dataset=train_ds,\n",
    "\n",
    "        eval_dataset=val_ds,\n",
    "\n",
    "        tokenizer=tokenizer,\n",
    "\n",
    "        compute_metrics=compute_metrics,\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "    # Calibrate\n",
    "\n",
    "    val_logits = trainer.predict(val_ds).predictions\n",
    "\n",
    "    val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "    thr = calibrate_thresholds(val_df[LABELS].values.astype(int), val_probs, LABELS)\n",
    "\n",
    "\n",
    "\n",
    "    # Test\n",
    "\n",
    "    test_logits = trainer.predict(test_ds).predictions\n",
    "\n",
    "    test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "\n",
    "    report_t = evaluate_all(\n",
    "\n",
    "        test_df[LABELS].values.astype(int),\n",
    "\n",
    "        test_probs,\n",
    "\n",
    "        LABELS,\n",
    "\n",
    "        thr,\n",
    "\n",
    "        title=\"[Transformer] Test metrics (calibrated)\",\n",
    "\n",
    "    )\n",
    "\n",
    "    Path(\"outputs/reports/transformer_report.txt\").write_text(report_t, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save\n",
    "\n",
    "    tdir = Path(\"outputs/models/transformer/final\")\n",
    "\n",
    "    tdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    trainer.model.save_pretrained(str(tdir))\n",
    "\n",
    "    tokenizer.save_pretrained(str(tdir))\n",
    "\n",
    "    (tdir / \"labels.txt\").write_text(\"\\n\".join(LABELS), encoding=\"utf-8\")\n",
    "\n",
    "    (tdir / \"thresholds.json\").write_text(json.dumps(thr, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Transformer artifacts saved to\", tdir)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Skipping transformer section (transformers/torch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def load_baseline(model_dir=Path(\"outputs/models/baseline\")) -> Tuple:\n",
    "    vec = joblib.load(model_dir / \"vectorizer.joblib\")\n",
    "    clf = joblib.load(model_dir / \"ovr_lr.joblib\")\n",
    "    labels = (model_dir / \"labels.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "    thresholds = json.loads((model_dir / \"thresholds.json\").read_text(encoding=\"utf-8\"))\n",
    "    return vec, clf, labels, thresholds\n",
    "\n",
    "def infer_baseline(texts: List[str], model_dir=Path(\"outputs/models/baseline\")):\n",
    "    vec, clf, labels, thresholds = load_baseline(model_dir)\n",
    "    X = vec.transform([normalize_text(t) for t in texts])\n",
    "    prob = clf.predict_proba(X)\n",
    "    preds = apply_thresholds(prob, thresholds, labels)\n",
    "    return labels, prob, preds\n",
    "\n",
    "def load_transformer(model_dir=Path(\"outputs/models/transformer/final\")):\n",
    "    tok = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(str(model_dir))\n",
    "    labels = (model_dir / \"labels.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "    thresholds = json.loads((model_dir / \"thresholds.json\").read_text(encoding=\"utf-8\"))\n",
    "    return tok, mdl, labels, thresholds\n",
    "\n",
    "def infer_transformer(texts: List[str], model_dir=Path(\"outputs/models/transformer/final\"), max_len=256):\n",
    "    tok, mdl, labels, thresholds = load_transformer(model_dir)\n",
    "    enc = tok(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = mdl(**{k: v for k,v in enc.items()})[0]\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = apply_thresholds(probs, thresholds, labels)\n",
    "    return labels, probs, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46b747",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infer_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m sample_texts = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm so happy with this patch and the dev team did an amazing job!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre such an idiot, go crawl back to your cave.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBruh.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m labels, probs, preds = \u001b[43minfer_baseline\u001b[49m(sample_texts)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text, prob_row, pred_row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sample_texts, probs, preds):\n\u001b[32m     12\u001b[39m     active = [lab \u001b[38;5;28;01mfor\u001b[39;00m lab, is_on \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, pred_row) \u001b[38;5;28;01mif\u001b[39;00m is_on]\n",
      "\u001b[31mNameError\u001b[39m: name 'infer_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"I'm so happy with this patch and the dev team did an amazing job!\",\n",
    "    \"You're such an idiot, go crawl back to your cave.\",\n",
    "    \"I'm a hater.\",\n",
    "    \"Bro I hate this game.\",\n",
    "    \"Bruh.\"\n",
    "]\n",
    "\n",
    "labels, probs, preds = infer_baseline(sample_texts)\n",
    "\n",
    "for text, prob_row, pred_row in zip(sample_texts, probs, preds):\n",
    "    active = [lab for lab, is_on in zip(labels, pred_row) if is_on]\n",
    "    print(\"TEXT:\", text)\n",
    "    print(\"Active labels:\", active or [\"none\"])\n",
    "    print(\"Probabilities:\", {lab: round(float(p), 3) for lab, p in zip(labels, prob_row)})\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
